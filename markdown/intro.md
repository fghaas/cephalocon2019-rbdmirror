<!-- .slide: data-timing="90" -->
# Geographical Redundancy with rbd-mirror
Best practices, performance  
recommendations, and pitfalls

* * *

Florian Haas | [@xahteiwi](https://twitter.com/xahteiwi)

Cephalocon 2019 | 2019-05-20

<!-- Note --> 
rbd-mirror is a Ceph feature that‚Äôs been around for 3 years ‚Äî since
the Jewel release ‚Äî as a means of asynchronously replicating RADOS
block device (RBD) content to a remote Ceph cluster.

That's all fair and good, I hear you cry, but how do I use the damn
thing?

* How exactly does the rbd-mirror daemon work?
* What's the difference between one-way and two-way mirroring?
* What authentication considerations apply?
* How do I deploy rbd-mirror in an automated fashion?
* How is mirroring related to RBD journaling?
* How does that affect RBD performance? 
* How do I integrate my mirrored devices into a cloud platform like
  OpenStack, so I can achieve true site-to-site redundancy and
  disaster recovery capability for persistent volumes?

Well it‚Äôs great you asked, because that‚Äôs what this talk is about.
I‚Äôm about to give you a run-down of the ins and outs of RBD mirroring,
I‚Äôll suggests best practices to deploy it, I‚Äôll outline performance
considerations, and I‚Äôll try to highlight traps and pitfalls to avoid
along the way.

My name is Florian, I‚Äôve worked with Ceph since about 2012, and my
Twitter handle is here on this slide, and also on all other slides in
the bottom-right corner. If you have questions, please feel free to
tweet them, include my handle, and if time permits, I‚Äôll be happy to
get to your question during the talk ‚Äî if not, I‚Äôll reply to your
question directly on Twitter, afterwards.


<!-- .slide: data-timing="15" -->
## RBD mirroring

... asynchronously replicates RBD data  
from one Ceph cluster to another. <!-- .element: class="fragment" --> 

<!-- Note -->
What‚Äôs RBD mirroring good for? RBD mirroring serves the purpose of

* **asynchronously** replicating RADOS Block Device (RBD) data from
one Ceph cluster to another.

Now to understand how and why that is useful, we‚Äôll have to back up a
bit and look at how replication in a Ceph cluster *normally* works.


<!-- .slide: data-background-image="images/osd-replication.svg" data-background-size="contain" -->
## Standard Ceph replication <!-- .element: class="hidden" --> 

<!-- Note -->
As an application writes data to a Ceph cluster, it always talks to a
single OSD (the **primary OSD**), which then takes care of replicating
the write to the other, non-primary OSDs. 

Consider the flying blue balls here representing I/O in flight: as
long as it‚Äôs not completed ‚Äî turned green ‚Äî *and reported as
such* from all OSDs, the application considers it not completed at all
and remains blue. Only when an acknowledgment from all OSDs has
arrived does the client consider the write operation complete, and
turns green itself.

This is what we refer to as **synchronous** replication.


<!-- .slide: data-background-image="images/osd-replication-slow.svg" data-background-size="contain" -->
## Slow OSD <!-- .element: class="hidden" --> 

<!-- Note -->
Synchronous replication is, inherently, latency-critical. If just one
OSD is slow, or slow *to get to*, then that potentially holds up your
entire application.

This is why, with very few and clearly delineated exceptions, you
should not attempt to do something like this:


<!-- .slide: data-background-image="images/map.svg" data-background-size="cover" -->
## Stretched cluster <!-- .element: class="hidden" --> 

<!-- Note -->
This approach is known as ‚Äústretched cluster‚Äù and means that you‚Äôre
operating a single Ceph cluster, stretched across several
physically separated locations or ‚Äúsites.‚Äù 

You should only do this if you have full control over your
site-to-site links, and you can ensure that your latency stays within
reasonable limits. Normally, this is only true for Ceph clusters where
all nodes are within the same city, **and** where you have dedicated
fiber connections between them.

However, building stretched clusters for Ceph becomes impossible once
you‚Äôre having to deal with ùíÑ.

* * *

Map of Barcelona: ¬© OpenStreetMap contributors.

OpenStreetMap data is available under the [Open Database
Licence](https://www.opendatacommons.org/licenses/odbl).


<!-- .slide: data-timing="15" -->
# ùíÑ
‚âà 299 792.5 km/s <!-- .element: class="fragment" --> 

<!-- Note -->
No, that‚Äôs not C the programming language, but c as in the speed of light.

Now the scientists and academics in the audience will forgive me the
cheap joke, because in fact we‚Äôre obviously talking about the speed of
light not in a vacuum, but in fibre-optic cables, [where it‚Äôs more
like 204,190.5
km/s](https://www.quora.com/What-is-precisely-the-speed-of-light-in-fiber-optics/answer/Steve-Blumenkranz).

But:


<!-- .slide: data-timing="60" -->
### Why long-distance Ceph clusters don‚Äôt work <!-- .element: class="hidden" --> 

Ethernet RTT ‚âà 0.1 ms <!-- .element: class="fragment" --> 

0.1 light-ms (in fibre) ‚âà 20 km <!-- .element: class="fragment" --> 

10 km = 20 km round-trip <!-- .element: class="fragment" --> 

100 km ‚âà 1 ms light round-trip <!-- .element: class="fragment" --> 

100 km = 10√ó Ethernet RTT <!-- .element: class="fragment" --> 

1,000 km = 100√ó Ethernet RTT <!-- .element: class="fragment" --> 

<!-- Note --> 
* Consider that a typical Ethernet round-trip time (ping time) is on
  the order of 100¬µs or 0.1 milliseconds.

* 0.1 *light*-milliseconds in fibre, that is to say the distance that
  light travels in a glass fibre in that time, is just 20 kilometers,
  or
  
* a round-trip of a link that‚Äôs 10 km one-way.

  And that‚Äôs assuming a theoretically perfect environment, where
  there‚Äôs **no** additional latency, other than that of light
  traveling ‚Äî in a straight-line fibre-optic link ‚Äî between the two
  locations. So you can‚Äôt ever assume LAN-like latency outside a 10-km
  radius.

* It also means if your cluster sites are just 100 km apart ‚Äî which
  doesn‚Äôt even cover the distance from here to the French border ‚Äî,
  
* your round-trip time increases tenfold versus a local Ethernet LAN.
  
* For 1000 km (a distance from here to Munich, or Lisbon) it‚Äôs
  100-fold.

So clearly, building stretched clusters is not an option if you want
to replicate your Ceph data on a continental, let alone
intercontinental scale.


<!-- .slide: data-background-image="images/multiple-clusters.svg" data-background-size="contain" -->
## RBD Multi-Site Replication <!-- .element: class="hidden" --> 

<!-- Note -->
So this is why, for long-distance replication, we must take a
different approach. And that is, we‚Äôre not doing replication at the
low, RADOS level, from OSD to OSD, but instead we‚Äôre doing it one
level up, at one of the abstraction layers *on top of* RADOS.

In this case, I am talking about the abstraction layer that‚Äôs most
important for virtualization use cases, namely RBD.

And, importantly, we need to be talking about replication that is
**asynchronous,** so we *don‚Äôt* have to worry about network latency
anymore.

~Now the first such Ceph application that got asynchronous replication
capability *wasn‚Äôt* RBD, it was RADOS Gateway (rgw). RADOS Gateway,
however, deals only with RESTful object data, and we needed something
to also replicate RBD block data, which we got in Jewel: RBD
mirroring.~

So, what we‚Äôd like to get is an RBD image whose changes are
*asynchronously* applied in a remote location. To get that capability,
we need to *track and record* those changes in the first place. To do
that, we use an RBD feature called **journaling.**
